<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[脑波网]]></title>
  <link href="http://dexterdeng.github.com/atom.xml" rel="self"/>
  <link href="http://dexterdeng.github.com/"/>
  <updated>2013-01-07T13:21:14+08:00</updated>
  <id>http://dexterdeng.github.com/</id>
  <author>
    <name><![CDATA[Dexter Deng]]></name>
    <email><![CDATA[dexterdeng@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[一点经验教训]]></title>
    <link href="http://dexterdeng.github.com/blog/2013/01/07/%5B%3F%5D-dian-jing-yan-jiao-xun/"/>
    <updated>2013-01-07T12:27:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2013/01/07/[?]-dian-jing-yan-jiao-xun</id>
    <content type="html"><![CDATA[<h3>流量刚过15k，一个新的里程碑</h3>

<p>坚持做两年的项目， 流量上了一个新阶段。</p>

<p>很多问题随之产生， 这里就最近的一些经验。稍微总结一下。</p>

<h3>数据库session表超大，导致dump文件超大，11G</h3>

<p>最近数据库dump文件超大，一直欣慰的以为数据量大了，实则可笑至极。</p>

<p>在解决这个问题过程中， 我找到几个方案。</p>

<h4>数据量真的庞大</h4>

<p>对数据做partition, 这样数据库更稳定。</p>

<h4>有很庞大的表</h4>

<p>如果数据不是很有价值，那么应该删除过期数据。
如果数据很有价值，那么应该分表了。</p>

<h3>Perconas是个好东西， 更省cpu</h3>

<p>以后有机会，要换这个数据库。</p>

<h3>数据库恢复时，手动restore 了一些表，</h3>

<p>发现一下行其实是有作用的，否则容易出现法语乱码:</p>

<blockquote><p>&#8211; MySQL dump 10.11
  &#8211;
  &#8211; Host: localhost    Database: blabla</p>

<hr />

<p>  &#8211; Server version5.0.92-log</p>

<p>/<em>!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT </em>/;
  /<em>!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS </em>/;
  /<em>!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION </em>/;
  /<em>!40101 SET NAMES utf8 </em>/;
  /<em>!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE </em>/;
  /<em>!40103 SET TIME_ZONE=&#8217;+00:00&#8217; </em>/;
  /<em>!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 </em>/;
  /<em>!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS,
  FOREIGN_KEY_CHECKS=0 </em>/;
  /<em>!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&#8217;NO_AUTO_VALUE_ON_ZERO&#8217;
  </em>/;
  /<em>!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 </em>/;</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mySQL优化 my.ini 配置说明]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/08/mysqlyou-hua-my-dot-ini-pei-zhi-shuo-ming/"/>
    <updated>2012-11-08T15:19:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/08/mysqlyou-hua-my-dot-ini-pei-zhi-shuo-ming</id>
    <content type="html"><![CDATA[<p>[mysqld]
port = 3306
serverid = 1</p>

<p>socket = /tmp/mysql.sock</p>

<p>skip-name-resolve</p>

<blockquote><p>禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！注：如果用winform连接mysql，加入此句速度会有很大的提升</p></blockquote>

<p>skip-locking</p>

<blockquote><p>避免MySQL的外部锁定，减少出错几率增强稳定性。</p></blockquote>

<p>back_log = 384</p>

<blockquote><p>指定MySQL可能的连接数量。当MySQL主线程在很短的时间内接收到非常多的连接请求，该参数生效，主线程花费很短的时间检查连接并且启动一个新线程。 back_log参数的值指出在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中。 如果系统在一个短时间内有很多连接，则需要增大该参数的值，该参数值指定到来的TCP/IP连接的侦听队列的大小。不同的操作系统在这个队列大小上有它自己的限制。 试图设定back_log高于你的操作系统的限制将是无效的。默认值为50。对于Linux系统推荐设置为小于512的整数。</p></blockquote>

<p>key_buffer_size = 32M</p>

<blockquote><p>key_buffer_size这对MyISAM表来说非常重要。如果只是使用MyISAM表，可以把它设置为可用内存的 30-40%。合理的值取决于索引大小、数据量以及负载 — 记住，MyISAM表会使用操作系统的缓存来缓存数据，因此需要留出部分内存给它们，很多情况下数据比索引大多了。尽管如此，需要总是检查是否所有的 key_buffer 都被利用了 — .MYI 文件只有 1GB，而 key_buffer 却设置为 4GB 的情况是非常少的。这么做太浪费了。如果你很少使用MyISAM表，那么也保留低于 16-32MB 的key_buffer_size 以适应给予磁盘的临时表索引所需。</p></blockquote>

<p>innodb_buffer_pool_size = 2.4G</p>

<blockquote><p>这对Innodb表来说非常重要。Innodb相比MyISAM表对缓冲更为敏感。MyISAM可以在默认的 key_buffer_size 设置下运行的可以，然而Innodb在默认的innodb_buffer_pool_size 设置下却跟蜗牛似的。由于Innodb把数据和索引都缓存起来，无需留给操作系统太多的内存，因此如果只需要用Innodb的话则可以设置它高达 70-80% 的可用内存。– 如果你的数据量不大，并且不会暴增，那么无需把innodb_buffer_pool_size 设置的太大了。</p></blockquote>

<p>innodb_additional_pool_size = 20M</p>

<blockquote><p>这个选项对性能影响并不太多，至少在有差不多足够内存可分配的操作系统上是这样。不过如果你仍然想设置为 20MB(或者更大)，因此就需要看一下Innodb其他需要分配的内存有多少。</p></blockquote>

<p>innodb_log_file_size = 512M</p>

<blockquote><p>在高写入负载尤其是大数据集的情况下很重要。这个值越大则性能相对越高，但是要注意到可能会增加恢复时间。我经常设置为64-512MB，根据服务器大小而异。</p></blockquote>

<p>innodb_log_buffer_size =16M</p>

<blockquote><p>默认的设置在中等强度写入负载以及较短事务的情况下，服务器性能还可以。如果存在更新操作峰值或者负载较大，就应该考虑加大它的值了。如果它的值设置太高了，可能会浪费内存 — 它每秒都会刷新一次，因此无需设置超过1秒所需的内存空间。通常8-16MB就足够了。越小的系统它的值越小。</p></blockquote>

<p>innodb_flush_logs_at_trx_commit = 2</p>

<blockquote><p>是否为Innodb比MyISAM慢1000倍而头大?看来也许你忘了修改这个参数了。默认值是 1，这意味着每次提交的更新事务(或者每个事务之外的语句)都会刷新到磁盘中，而这相当耗费资源，尤其是没有电池备用缓存时。很多应用程序，尤其是从 MyISAM转变过来的那些，把它的值设置为 2 就可以了，也就是不把日志刷新到磁盘上，而只刷新到操作系统的缓存上。日志仍然会每秒刷新到磁盘中去，因此通常不会丢失每秒1-2次更新的消耗。如果设置为0就快很多了，不过也相对不安全了 — MySQL服务器崩溃时就会丢失一些事务。设置为2指挥丢失刷新到操作系统缓存的那部分事务。</p></blockquote>

<p>max_allowed_packet = 4M
thread_stack = 256K
table_cache = 128K
sort_buffer_size = 6M</p>

<blockquote><p>查询排序时所能使用的缓冲区大小。注意：该参数对应的分配内存是每连接独占！如果有100个连接，那么实际分配的总共排序缓冲区大小为100 × 6 ＝ 600MB。所以，对于内存在4GB左右的服务器推荐设置为6-8M。</p></blockquote>

<p>read_buffer_size = 4M</p>

<blockquote><p>读查询操作所能使用的缓冲区大小。和sort_buffer_size一样，该参数对应的分配内存也是每连接独享！</p></blockquote>

<p>join_buffer_size = 8M</p>

<blockquote><p>联合查询操作所能使用的缓冲区大小，和sort_buffer_size一样，该参数对应的分配内存也是每连接独享！</p></blockquote>

<p>myisam_sort_buffer_size = 64M
table_cache = 512</p>

<blockquote><p>打开一个表的开销可能很大。例如MyISAM把MYI文件头标志该表正在使用中。你肯定不希望这种操作太频繁，所以通常要加大缓存数量，使得足以最大限度地缓存打开的表。它需要用到操作系统的资源以及内存，对当前的硬件配置来说当然不是什么问题了。如果你有200多个表的话，那么设置为 1024 也许比较合适(每个线程都需要打开表)，如果连接数比较大那么就加大它的值。我曾经见过设置为100,000的情况。</p></blockquote>

<p>thread_cache_size = 64</p>

<blockquote><p>线程的创建和销毁的开销可能很大，因为每个线程的连接/断开都需要。我通常至少设置为 16。如果应用程序中有大量的跳跃并发连接并且 Threads_Created 的值也比较大，那么我就会加大它的值。它的目的是在通常的操作中无需创建新线程。</p></blockquote>

<p>query_cache_size = 64M</p>

<blockquote><p>指定MySQL查询缓冲区的大小。可以通过在MySQL控制台执行以下命令观察：</p>

<blockquote><p>SHOW VARIABLES LIKE ‘%query_cache%’;
SHOW STATUS LIKE ‘Qcache%’;
如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况；如果Qcache_hits的值非常大，则表明查询缓冲使用非常频繁，如果该值较小反而会影响效率，那么可以考虑不用查询缓冲；Qcache_free_blocks，如果该值非常大，则表明缓冲区中碎片很多。</p></blockquote></blockquote>

<p>tmp_table_size = 256M
max_connections = 768</p>

<blockquote><p>指定MySQL允许的最大连接进程数。如果在访问论坛时经常出现Too Many Connections的错误提 示，则需要增大该参数值。</p></blockquote>

<p>max_connect_errors = 10000000
wait_timeout = 10</p>

<blockquote><p>指定一个请求的最大连接时间，对于4GB左右内存的服务器可以设置为5-10。</p></blockquote>

<p>thread_concurrency = 8</p>

<blockquote><p>该参数取值为服务器逻辑CPU数量×2，在本例中，服务器有2颗物理CPU，而每颗物理CPU又支持H.T超线程，所以实际取值为4 × 2 ＝ 8</p></blockquote>

<p>skip-networking</p>

<blockquote><p>开启该选项可以彻底关闭MySQL的TCP/IP连接方式，如果WEB服务器是以远程连接的方式访问MySQL数据库服务器则不要开启该选项！否则将无法正常连接！</p></blockquote>

<p>show status 命令</p>

<p>含义如下:</p>

<ul>
<li>aborted_clients 客户端非法中断连接次数</li>
<li>aborted_connects 连接mysql失败次数</li>
<li>com_xxx xxx命令执行次数,有很多条</li>
<li>connections 连接mysql的数量</li>
<li>Created_tmp_disk_tables 在磁盘上创建的临时表</li>
<li>Created_tmp_tables 在内存里创建的临时表</li>
<li>Created_tmp_files 临时文件数</li>
<li>Key_read_requests The number of requests to read a key block from the cache</li>
<li>Key_reads The number of physical reads of a key block from disk</li>
<li>Max_used_connections 同时使用的连接数</li>
<li>Open_tables 开放的表</li>
<li>Open_files 开放的文件</li>
<li>Opened_tables 打开的表</li>
<li>Questions 提交到server的查询数</li>
<li>Sort_merge_passes 如果这个值很大,应该增加my.cnf中的sort_buffer值</li>
<li>Uptime 服务器已经工作的秒数</li>
</ul>


<h3>提升性能的建议:</h3>

<ol>
<li>如果opened_tables太大,应该把my.cnf中的table_cache变大</li>
<li>如果Key_reads太大,则应该把my.cnf中key_buffer_size变大.可以用Key_reads/Key_read_requests计算出cache失败率</li>
<li>如果Handler_read_rnd太大,则你写的SQL语句里很多查询都是要扫描整个表,而没有发挥索引的键的作用</li>
<li>如果Threads_created太大,就要增加my.cnf中thread_cache_size的值.可以用Threads_created/Connections计算cache命中率</li>
<li>如果Created_tmp_disk_tables太大,就要增加my.cnf中tmp_table_size的值,用基于内存的临时表代替基于磁盘的</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to enhance mysql performance]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/08/how-to-enhance-mysql-performance/"/>
    <updated>2012-11-08T11:36:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/08/how-to-enhance-mysql-performance</id>
    <content type="html"><![CDATA[<h3>1. use <code>show processlist</code></h3>

<pre><code>show processlist
</code></pre>

<p>it can  check the connections,  and find the slow sql.  You can add index to some columns if needed</p>

<h3>2. check <code>/var/lib/mysql/slow.log</code></h3>

<pre><code>cat /var/lib/mysql/slow.log
</code></pre>

<p>it can  check the connections,  and find the slow sql.  You can add index to some columns if needed</p>

<p>e.g.</p>

<pre><code>SELECT DISTINCT(events.id), events.* FROM `events`   left join  event_users on event_users.event_id = events.id  WHERE ((events.user_id = 444 or (event_users.user_id = 444 and event_users.status != 0)) and (events.end_at &lt; '2012-11-07'  or (events.end_at  is null and events.start_at  &lt; '2012-11-07' )))  LIMIT 0, 20;
</code></pre>

<p>it took 5 s for the first running.</p>

<pre><code>show index from events
</code></pre>

<p>I got this:</p>

<pre><code>    +--------+------------+---------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+
| Table  | Non_unique | Key_name                  | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment |
+--------+------------+---------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+
| events |          0 | PRIMARY                   |            1 | id          | A         |        2180 |     NULL | NULL   |      | BTREE      |         | 
| events |          1 | index_events_on_permalink |            1 | permalink   | A         |        2180 |     NULL | NULL   | YES  | BTREE      |         | 
+--------+------------+---------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+
2 rows in set (0.00 sec)


mysql&gt; show index from event_users;
+-------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+
| Table       | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment |
+-------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+
| event_users |          0 | PRIMARY  |            1 | id          | A         |        7671 |     NULL | NULL   |      | BTREE      |         | 
+-------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+
</code></pre>

<p>I decide to add index to event_users&#8217;s event_id, user_id, events.user_id</p>

<pre><code>ALTER TABLE `events` ADD INDEX ( `user_id` )
ALTER TABLE `event_users` ADD INDEX ( `user_id` )
ALTER TABLE `event_users` ADD INDEX ( `event_id` )
</code></pre>

<h3>2. use <code>mysqltuner.pl</code>###</h3>

<p><code>mysqltuner.pl</code> is a perl script. it can connect the mysql and check
issues, and recommend you how to enhance it.</p>

<pre><code>$ perl mysqltuner.pl 

 &gt;&gt;  MySQLTuner 1.2.0 - Major Hayden &lt;major@mhtx.net&gt;
 &gt;&gt;  Bug reports, feature requests, and downloads at http://mysqltuner.com/
 &gt;&gt;  Run with '--help' for additional options and output filtering
Please enter your MySQL administrative login: root
Please enter your MySQL administrative password: 

-------- General Statistics --------------------------------------------------
[--] Skipped version check for MySQLTuner script
[OK] Currently running supported MySQL version 5.0.92-log
[OK] Operating on 64-bit architecture

-------- Storage Engine Statistics -------------------------------------------
[--] Status: -Archive -BDB -Federated +InnoDB -ISAM -NDBCluster 
[--] Data in MyISAM tables: 105M (Tables: 55)
[--] Data in InnoDB tables: 18G (Tables: 938)
[!!] Total fragmented tables: 6

-------- Security Recommendations  -------------------------------------------
[OK] All database users have passwords assigned

-------- Performance Metrics -------------------------------------------------
[--] Up for: 4d 20h 36m 22s (38M q [91.609 qps], 65K conn, TX: 78B, RX: 5B)
[--] Reads / Writes: 88% / 12%
[--] Total buffers: 3.3G global + 42.2M per thread (100 max threads)
[OK] Maximum possible memory usage: 7.5G (31% of installed RAM)
[OK] Slow queries: 0% (77/38M)
[OK] Highest usage of available connections: 43% (43/100)
[OK] Key buffer size / total MyISAM indexes: 140.0M/143.8M
[OK] Key buffer hit rate: 99.9% (218M cached / 122K reads)
[OK] Query cache efficiency: 36.7% (12M cached / 32M selects)
[!!] Query cache prunes per day: 2340785
[OK] Sorts requiring temporary tables: 0% (5K temp sorts / 2M sorts)
[!!] Joins performed without indexes: 109640
[!!] Temporary tables created on disk: 29% (700K on disk / 2M total)
[OK] Thread cache hit rate: 99% (43 created / 65K connections)
[!!] Table cache hit rate: 0% (100 open / 27K opened)
[OK] Open file limit used: 0% (15/20K)
[OK] Table locks acquired immediately: 99% (30M immediate / 30M locks)
[!!] InnoDB data size / buffer pool: 18.5G/3.0G

-------- Recommendations -----------------------------------------------------
General recommendations:
    Run OPTIMIZE TABLE to defragment tables for better performance
    Adjust your join queries to always utilize indexes
    When making adjustments, make tmp_table_size/max_heap_table_size equal
    Reduce your SELECT DISTINCT queries without LIMIT clauses
    Increase table_cache gradually to avoid file descriptor limits
Variables to adjust:
    query_cache_size (&gt; 100M)
    join_buffer_size (&gt; 32.0M, or always use indexes with joins)
    tmp_table_size (&gt; 100M)
    max_heap_table_size (&gt; 100M)
    table_cache (&gt; 100)
    innodb_buffer_pool_size (&gt;= 18G)
</code></pre>

<h4>1. <code>Run OPTIMIZE TABLE to defragment tables for better performance</code></h4>

<p>Run this command:</p>

<pre><code>mysqlcheck -u root --auto-repair --optimize --all-databases
</code></pre>

<p>You can usually fix the table by issuing:</p>

<pre><code>mysqlcheck -A -r -p
</code></pre>

<p>Restart MYSQL Server:</p>

<pre><code>/etc/init.d/mysql restart
</code></pre>

<p>use https://tools.percona.com/wizard/ to generate a conf file.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to integrate mmseg with elasticsearch]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/06/how-to-integrate-mmseg-with-elasticsearch/"/>
    <updated>2012-11-06T11:46:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/06/how-to-integrate-mmseg-with-elasticsearch</id>
    <content type="html"><![CDATA[<h3>install elasticsearch-analysis-mmseg</h3>

<pre><code>cd /usr/local/elasticsearch
sudo bin/plugin -install medcl/elasticsearch-analysis-mmseg/1.1.0  
</code></pre>

<h3>configure</h3>

<pre><code>vi /etc/elasticsearch/elasticsearch.yml
</code></pre>

<p>add these lines in the elasticsearch</p>

<pre><code>index:
  analysis:
    analyzer:
      mmseg
        alias: [news_analyzer, mmseg_analyzer]
        type: org.elasticsearch.index.analysis.MMsegAnalyzerProvider
    tokenizer:
      mmseg_maxword:
        type: mmseg
        seg_type: "max_word"
      mmseg_complex:
        type: mmseg
        seg_type: "complex"
      mmseg_simple:
        type: mmseg
        seg_type: "simple"
index.analysis.analyzer.default.type : "mmseg"
</code></pre>

<p>it means define analyzer and tokenizer. and the default analyzer</p>

<h3>restart elasticsearch</h3>

<pre><code>/etc/init.d/elasticsearch stop
/etc/init.d/elasticsearch start
</code></pre>

<h3>How to use it</h3>

<pre><code>tire.mapping do
  indexes :id,             type: 'integer', :index =&gt; :not_analyzed
  indexes :name,           type: "string", index: "analyzed", analyzer: "mmseg"
  indexes :pinyin,         type: 'string'
  indexes :created_at,     type: 'date'
  indexes :updated_at,     type: 'date'
end       
</code></pre>

<h3>regenerate index</h3>

<pre><code>$ RAILS_ENV=production bundle exec rake environment tire:import CLASS='Article' FORCE=true
</code></pre>

<p>http://www.elasticsearch.org/guide/reference/modules/plugins.html</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[delete blank lines and replace with enter]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/05/delete-blank-lines-and-replace-with-enter/"/>
    <updated>2012-11-05T17:00:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/05/delete-blank-lines-and-replace-with-enter</id>
    <content type="html"><![CDATA[<h3>How to replace , with enter</h3>

<p>e.g. How to replace <code>aaa,bbb,ccc</code> with</p>

<pre><code>aaa
bbb
ccc
</code></pre>

<p>open it with <code>mvim</code></p>

<pre><code>:%s/,/^M/g
# you need to type CTRL-V &lt;CR&gt; to get a ^M here
</code></pre>

<h3>How to remove blank lines</h3>

<pre><code>:%g/^\s*$/d
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[skip vadation or skip flilter]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/03/skip-vadation-or-skip-flilter/"/>
    <updated>2012-11-03T17:09:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/03/skip-vadation-or-skip-flilter</id>
    <content type="html"><![CDATA[<h3>run all validation and filters</h3>

<pre><code>@user.save
</code></pre>

<h3>run  no validation but all filters</h3>

<pre><code>@user.save(false)
</code></pre>

<h3>run  no validation and no filters</h3>

<pre><code>@user.send(:update_without_callbacks)
</code></pre>

<p>since it is private, you need to use send</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Never call save/update in before/after_save filter]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/03/never-call-save-update-attribute-update-attributes-in-after-slash-after-save-filter/"/>
    <updated>2012-11-03T17:02:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/03/never-call-save-update-attribute-update-attributes-in-after-slash-after-save-filter</id>
    <content type="html"><![CDATA[<p>Check two source code first:</p>

<pre><code>def update_attribute(name, value)
  send(name.to_s + '=', value)
  save(false)
end
</code></pre>

<p>and</p>

<pre><code> def update_attributes(attributes)
   self.attributes = attributes
   save
 end
</code></pre>

<p>Check the source code, these 2 <code>update</code> methods contain <code>save</code>. in fact,
<code>update</code>  always means <code>save</code>.  these methods can&#8217;t be in <code>after_save</code>,
<code>before_save</code>. as  if you call <code>save</code>, <code>save</code> trigger  <code>before_save/after_save</code>,
and <code>before_save/after_save</code> trigger <code>save</code> again. &#8230;  No ending.</p>

<p>This is very important.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ Linux Grep OR , Grep AND, Grep NOT]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/03/linux-grep-or/"/>
    <updated>2012-11-03T00:11:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/03/linux-grep-or</id>
    <content type="html"><![CDATA[<p><strong>Question:</strong> Can you explain how to use <code>OR</code>, <code>AND</code> and <code>NOT</code> operators in Unix grep command with some examples?</p>

<p><strong>Answer:</strong> In grep, we have options equivalent to <code>OR</code> and <code>NOT</code> operators. There is no grep <code>AND</code> opearator. But, you can simulate <code>AND</code> using patterns. The examples mentioned below will help you to understand how to use <code>OR</code>, <code>AND</code> and <code>NOT</code> in Linux grep command.</p>

<p>The following employee.txt file is used in the following examples.</p>

<pre><code>$ cat employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
400  Nisha   Manager    Marketing   $9,500
500  Randy   Manager    Sales       $6,000
</code></pre>

<p>You already knew that grep is extremely powerful based on these grep command examples.</p>

<h2>Grep OR Operator</h2>

<p>Use any one of the following 4 methods for grep OR. I prefer method number 3 mentioned below for grep OR operator.</p>

<h3>1. Grep OR Using \|</h3>

<p>If you use the grep command without any option, you need to use <code>\|</code> to separate multiple patterns for the or condition.</p>

<pre><code>grep 'pattern1\|pattern2' filename
</code></pre>

<p>For example, grep either Tech or Sales from the employee.txt file. Without the back slash in front of the pipe, the following will not work.</p>

<pre><code>$ grep 'Tech\|Sales' employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
</code></pre>

<h3>2. Grep OR Using -E</h3>

<p><code>grep -E</code> option is for extended regexp. If you use the grep command with <code>-E</code> option, you just need to use | to separate multiple patterns for the or condition.</p>

<pre><code>grep -E 'pattern1|pattern2' filename
</code></pre>

<p>For example, grep either Tech or Sales from the employee.txt file. Just use the | to separate multiple OR patterns.</p>

<pre><code>$ grep -E 'Tech|Sales' employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
</code></pre>

<h3>3. Grep OR Using egrep</h3>

<p>egrep is exactly same as <code>grep -E</code>. So, use egrep (without any option) and separate multiple patterns for the or condition.</p>

<pre><code>egrep 'pattern1|pattern2' filename
</code></pre>

<p>For example, grep either Tech or Sales from the employee.txt file. Just use the | to separate multiple OR patterns.</p>

<pre><code>$ egrep 'Tech|Sales' employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
</code></pre>

<h3>4. Grep OR Using grep -e</h3>

<p>Using grep -e option you can pass only one parameter. Use multiple <code>-e</code> option in a single command to use multiple patterns for the or condition.</p>

<pre><code>grep -e pattern1 -e pattern2 filename
</code></pre>

<p>For example, grep either Tech or Sales from the employee.txt file. Use multiple <code>-e</code> option with grep for the multiple OR patterns.</p>

<pre><code>$ grep -e Tech -e Sales employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
Grep AND
</code></pre>

<h3>5. Grep AND using <code>-E ‘pattern1.*pattern2'</code></h3>

<p>There is no AND operator in grep. But, you can simulate AND using grep -E option.</p>

<pre><code>grep -E 'pattern1.*pattern2' filename
grep -E 'pattern1.*pattern2|pattern2.*pattern1' filename
</code></pre>

<p>The following example will grep all the lines that contain both “Dev” and “Tech” in it (in the same order).</p>

<pre><code>$ grep -E 'Dev.*Tech' employee.txt
200  Jason   Developer  Technology  $5,500
</code></pre>

<p>The following example will grep all the lines that contain both “Manager” and “Sales” in it (in any order).</p>

<pre><code>$ grep -E 'Manager.*Sales|Sales.*Manager' employee.txt
</code></pre>

<p>Note: Using regular expressions in grep is very powerful if you know how to use it effectively.</p>

<h3>6. Grep AND using Multiple grep command</h3>

<p>You can also use multiple grep command separated by pipe to simulate AND scenario.</p>

<pre><code>grep -E 'pattern1' filename | grep -E 'pattern2'
</code></pre>

<p>The following example will grep all the lines that contain both “Manager” and “Sales” in the same line.</p>

<pre><code>$ grep Manager employee.txt | grep Sales
100  Thomas  Manager    Sales       $5,000
500  Randy   Manager    Sales       $6,000
Grep NOT
</code></pre>

<h3>7. Grep NOT using grep -v</h3>

<p>Using grep <code>-v</code> you can simulate the NOT conditions. <code>-v</code> option is for invert match. i.e It matches all the lines except the given pattern.</p>

<pre><code>grep -v 'pattern1' filename
</code></pre>

<p>For example, display all the lines except those that contains the keyword “Sales”.</p>

<pre><code>$ grep -v Sales employee.txt
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
400  Nisha   Manager    Marketing   $9,500
</code></pre>

<p>You can also combine NOT with other operator to get some powerful combinations.</p>

<p>For example, the following will display either Manager or Developer (bot ignore Sales).</p>

<pre><code>$ egrep 'Manager|Developer' employee.txt | grep -v Sales
200  Jason   Developer  Technology  $5,500
400  Nisha   Manager    Marketing   $9,500
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Integrate tire elasticsearch on bad server]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/11/01/integrate-tire-plus-elasticsearch-on-bad-server/"/>
    <updated>2012-11-01T13:24:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/11/01/integrate-tire-plus-elasticsearch-on-bad-server</id>
    <content type="html"><![CDATA[<p>Installing the JRE
Type the following commands into your SSH terminal.</p>

<pre><code>apt-get install openjdk-6-jdk 
</code></pre>

<p>You should also have JDK 6 or above installed.</p>

<pre><code>$ curl -k -L -o elasticsearch-0.19.0.tar.gz http://github.com/downloads/elasticsearch/elasticsearch/elasticsearch-0.19.0.tar.gz
$ tar -zxvf elasticsearch-0.19.0.tar.gz
$ ./elasticsearch-0.19.0/bin/elasticsearch -f
</code></pre>

<p>How to make it manage by ubuntu better.</p>

<pre><code>$ cp ./elasticsearch-0.19 /user/local/elasticsearch
</code></pre>

<p>make <code>/etc/init.d/elasticsearch</code></p>

<pre><code>$ vi /etc/init.d/elasticsearch


### END INIT INFO
ES_HOME=/usr/local/elasticsearch
export ES_MIN_MEM=32m
export ES_MAX_MEM=128m
DAEMON=$ES_HOME/bin/elasticsearch
NAME=elasticsearch
DESC=elasticsearch
PID_FILE=/var/run/$NAME.pid
LOG_DIR=/var/log/$NAME
DATA_DIR=/var/lib/$NAME
WORK_DIR=/tmp/$NAME
CONFIG_FILE=/etc/$NAME/elasticsearch.yml
DAEMON_OPTS="-p $PID_FILE -Des.config=$CONFIG_FILE -Des.path.home=$ES_HOME -Des.path.logs=$LOG_DIR -        Des.path.data=$DATA_DIR -Des.path.work=$WORK_DIR"


test -x $DAEMON || exit 0

set -e

case "$1" in
  start)
    echo -n "Starting $DESC: "
    mkdir -p $LOG_DIR $DATA_DIR $WORK_DIR
    if start-stop-daemon --start --pidfile $PID_FILE --startas $DAEMON -- $DAEMON_OPTS
    then
        echo "started."
    else
        echo "failed."
    fi
    ;;
  stop)
    echo -n "Stopping $DESC: "
    if start-stop-daemon --stop --pidfile $PID_FILE
    then
        echo "stopped."
    else
        echo "failed."
    fi
    ;;
  restart|force-reload)
    ${0} stop
    sleep 0.5
    ${0} start
    ;;
  *)
    N=/etc/init.d/$NAME
    echo "Usage: $N {start|stop|restart|force-reload}" &gt;&amp;2
    exit 1
    ;;
esac
exit 0
</code></pre>

<p>To a number suitable for the size of your forum.</p>

<p>I reccomend approximately <code>256mb</code> for the MIN_MEM and <code>1 GB</code> for the MAX_MEM per <code>1 million</code> posts on your forum.</p>

<pre><code>1 Million Posts: 256m:1g
2 Million Posts: 512m:2g
3 Million Posts: 768m:3g
4 Million Posts: 1024m:4g
etc
</code></pre>

<p>This will not mean the service will use all that available memory, however it will have it at its disposal if required.</p>

<p>So for example a <code>3 Million</code> Post forum would edit</p>

<pre><code>export ES_MIN_MEM=768m
export ES_MAX_MEM=3g
</code></pre>

<p>make the script executable and start automaticly.</p>

<pre><code>chmod a+x /etc/init.d/elasticsearch 
update-rc.d elasticsearch defaults
/etc/init.d/elasticsearch start

vi /etc/elasticsearch/elasticsearch.yml
</code></pre>

<p> and add this</p>

<pre><code>#/etc/elasticsearch/elasticsearch.yml
network :
  host :  127.0.0.1
path:
  logs: /var/log/elasticsearch
  data: /var/data/elasticsearch

cluster.name: PUT-SOMETHING-UNIQUE-HERE
</code></pre>

<p>make data folder</p>

<pre><code>mkdir -p  /var/data/elasticsearch
</code></pre>

<p>make sure  <code>127.0.0.1 localhost</code> in your <code>/ect/hosts</code>, this is very important, it&#8217;s waste of my time.</p>

<p>now you can start to build the index.</p>

<pre><code>RAILS_ENV=production rake environment tire:import CLASS='List'
</code></pre>

<p>on mac, there is  a simpler way:</p>

<pre><code>brew install elasticsearch
elasticsearch -f -D es.config=/usr/local/Cellar/elasticsearch/0.18.5/config/elasticsearch.yml
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[add user to group]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/10/30/add-user-to-group/"/>
    <updated>2012-10-30T16:55:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/10/30/add-user-to-group</id>
    <content type="html"><![CDATA[<p>How can I add a user to a group under Linux operating system?</p>

<p>You can use the useradd or usermod commands to add a user to a group. The useradd command creates a new user or update default new user information. The usermod command modifies a user account i.e. it is useful to add user to existing group. There are two types of group. First is primary user group and other is secondary group. All user account related information is stored in <code>/etc/passwd</code>, <code>/etc/shadow</code> and <code>/etc/group</code> files to store user information.</p>

<h3>useradd Example - Add A New User To Secondary Group</h3>

<p>You need to the useradd command to add new users to existing group (or create a new group and then add user). If group does not exist, create it. The syntax is as follows:</p>

<pre><code>useradd -G {group-name} username
</code></pre>

<p>In this example, create a new user called vivek and add it to group called developers. First login as a root user (make sure group developers exists), enter:</p>

<pre><code># grep developers /etc/group
</code></pre>

<p>Output:</p>

<pre><code>developers:x:1124:
</code></pre>

<p>If you do not see any output then you need to add group developers using groupadd command:</p>

<pre><code># groupadd developers
</code></pre>

<p>Next, add a user called vivek to group developers:</p>

<pre><code># useradd -G developers vivek
</code></pre>

<p>Setup password for user vivek:</p>

<pre><code># passwd vivek
</code></pre>

<p>Ensure that user added properly to group developers:</p>

<pre><code># id vivek
</code></pre>

<p>Output:</p>

<pre><code>uid=1122(vivek) gid=1125(vivek) groups=1125(vivek),1124(developers)
</code></pre>

<p>Please note that capital <code>G (-G)</code> option add user to a list of supplementary groups. Each group is separated from the next by a comma, with no intervening whitespace. For example, add user jerry to groups admins, ftp, www, and developers, enter:</p>

<pre><code># useradd -G admins,ftp,www,developers jerry
</code></pre>

<h3>useradd example - Add a new user to primary group</h3>

<p>To add a user tony to group developers use following command:</p>

<pre><code># useradd -g developers tony
# id tony
</code></pre>

<p>Sample outputs:</p>

<pre><code>uid=1123(tony) gid=1124(developers) groups=1124(developers)
</code></pre>

<p>Please note that small -g option add user to initial login group (primary group). The group name must exist. A group number must refer to an already existing group.</p>

<pre><code>usermod example - Add a existing user to existing group
</code></pre>

<p>Add existing user tony to ftp supplementary/secondary group with usermod command using -a option ~ i.e. add the user to the supplemental group(s). Use only with -G option :</p>

<pre><code># usermod -a -G ftp tony
</code></pre>

<p>Change existing user tony primary group to www:</p>

<pre><code># usermod -g www tony
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL 'Incorrect key file for table' error]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/10/30/mysql-incorrect-key-file-for-table-error/"/>
    <updated>2012-10-30T02:47:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/10/30/mysql-incorrect-key-file-for-table-error</id>
    <content type="html"><![CDATA[<p>When saving a record to a MySQL table the other day I got the error message &#8220;Incorrect key file for table &#8216;mytable&#8217;; try to repair it&#8221;. I am uncertain why the error occured and how to ensure it doesn&#8217;t happen again in the future but a quick fix for the time being is simple.</p>

<h2>How to repair a MySQL table</h2>

<p>All you need to do is to repair the table by running the following SQL command, where &#8220;mytable&#8221; is the name of the table that gave the error:</p>

<pre><code>REPAIR TABLE `mytable`;
</code></pre>

<p>You can run this from e.g. the MySQL CLI or phpMyAdmin. From phpMyAdmin select the table, then &#8220;Operations&#8221; from the navigation tabs in the right frame above the table info; then &#8220;Repair Table&#8221; from the &#8220;Table maintenance&#8221; options at the bottom of the page.</p>

<h2>When the table is /tmp/#sql_xxx_x.MYI</h2>

<p>If the error looks like &#8220;Incorrect key file for table &#8216;/tmp/#sql_xxx_x.MYI&#8217;; try to repair it&#8221; where it refers to a temporary location on the filesystem, it&#8217;s likely you&#8217;ve run out of diskspace. Read my follow up post for more information.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开博了，记录我的脑波]]></title>
    <link href="http://dexterdeng.github.com/blog/2012/10/23/kai-bo-liao-%2Cji-lu-wo-de-nao-bo-.-./"/>
    <updated>2012-10-23T16:21:00+08:00</updated>
    <id>http://dexterdeng.github.com/blog/2012/10/23/kai-bo-liao-,ji-lu-wo-de-nao-bo-.-.</id>
    <content type="html"><![CDATA[<p>开博了，记录我的脑波。。</p>
]]></content>
  </entry>
  
</feed>
